import os
import json
import uuid
import uvicorn
import asyncio
from urllib.parse import urlparse
from fastapi import FastAPI
from dotenv import load_dotenv
from datetime import datetime

from a2a.server.apps.jsonrpc.fastapi_app import A2AFastAPIApplication
from a2a.server.agent_execution import AgentExecutor
from a2a.server.request_handlers.default_request_handler import DefaultRequestHandler
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.server.agent_execution.context import RequestContext
from a2a.types import AgentCard

from openai import AsyncOpenAI

# Load environment variables from .env file
load_dotenv()

# Initialize OpenAI client (optional)
has_openai_key = (
    os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "dummy-key"
)
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY") or "dummy-key")

# Load agent card from .agent-card.json
with open(".agent-card.json", "r") as f:
    agent_card_dict = json.load(f)

# Override URL from environment if present
if os.getenv("AGENT_URL"):
    agent_card_dict["url"] = os.getenv("AGENT_URL")

# Ensure capabilities are set
if "capabilities" not in agent_card_dict:
    agent_card_dict["capabilities"] = {}
agent_card_dict["capabilities"]["streaming"] = True
agent_card_dict["capabilities"]["pushNotifications"] = True

# Convert dict to AgentCard object
agent_card = AgentCard.model_validate(agent_card_dict)


class Executor(AgentExecutor):
    """
    Implements the core agent logic for the basic template.
    Demonstrates:
    1. Task creation and management (A2A Task Protocol)
    2. Streaming status updates (submitted -> working -> completed)
    3. Generating and sending Artifacts
    4. Handling Cancellation
    """
    
    def __init__(self):
        self.cancelled_tasks = set()

    async def execute(
        self, request_context: RequestContext, event_bus
    ):
        task_id = request_context.task_id
        context_id = request_context.context_id
        user_message = request_context.user_message
        task = request_context.task

        try:
            # 1. Initialize Task if needed
            if not task:
                initial_task = {
                    "kind": "task",
                    "id": task_id,
                    "contextId": context_id,
                    "status": {
                        "state": "submitted",
                        "timestamp": datetime.utcnow().isoformat() + "Z",
                    },
                    "history": [user_message]
                }
                event_bus.publish(initial_task)

            user_text = next(
                (p["text"] for p in user_message["parts"] if p["kind"] == "text"), ""
            )
            print(f'[{task_id}] üì© Message received: "{user_text}"')

            # 2. Update Status to 'working'
            working_update = {
                "kind": "status-update",
                "taskId": task_id,
                "contextId": context_id,
                "status": {"state": "working", "timestamp": datetime.utcnow().isoformat() + "Z"},
                "final": False
            }
            event_bus.publish(working_update)

            # Check for cancellation
            if task_id in self.cancelled_tasks:
                return self._handle_cancelled(task_id, context_id, event_bus)

            response_text = ""
            
            if has_openai_key:
                try:
                    print(f"[{task_id}] üß† Calling OpenAI to generate response...")
                    
                    # Send an artifact to show "thinking"
                    event_bus.publish({
                        "kind": "artifact-update",
                        "taskId": task_id,
                        "contextId": context_id,
                        "artifact": {
                            "artifactId": f"thought-{int(datetime.utcnow().timestamp())}",
                            "name": "Thought Process",
                            "parts": [{"kind": "text", "text": "Analyzing your request..."}]
                        }
                    })

                    response = await openai_client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=[
                            {
                                "role": "system",
                                "content": f"You are a helpful assistant named {agent_card.name}.",
                            },
                            {"role": "user", "content": user_text},
                        ],
                        # We could enable streaming here and publish partial updates if A2A supports it
                        # For now, we await the full response to keep it simple, but check cancellation
                    )
                    
                    if task_id in self.cancelled_tasks:
                        return self._handle_cancelled(task_id, context_id, event_bus)
                        
                    response_text = response.choices[0].message.content
                    
                except Exception as e:
                    print(f"[{task_id}]  OpenAI error: {e}")
                    response_text = self._generate_fallback_response(user_text)
            else:
                # Simulate work
                print(f"[{task_id}] ü§ñ Generating fallback response (simulating work)...")
                for _ in range(10): # 1 second delay split into checks
                    if task_id in self.cancelled_tasks:
                        return self._handle_cancelled(task_id, context_id, event_bus)
                    await asyncio.sleep(0.1)
                    
                response_text = self._generate_fallback_response(user_text)

            if task_id in self.cancelled_tasks:
                return self._handle_cancelled(task_id, context_id, event_bus)

            # 3. Publish Final Response
            response_message = {
                "kind": "message",
                "messageId": str(uuid.uuid4()),
                "role": "agent",
                "parts": [{"kind": "text", "text": response_text or ""}],
                "contextId": context_id,
            }
            event_bus.publish(response_message)

            # 4. Mark Task as Completed
            completed_update = {
                "kind": "status-update",
                "taskId": task_id,
                "contextId": context_id,
                "status": {"state": "completed", "timestamp": datetime.utcnow().isoformat() + "Z"},
                "final": True,
            }
            event_bus.publish(completed_update)
            print(f"[{task_id}] üì§ Response sent and task completed.")

        except Exception as e:
            print(f"[{task_id}] üí• Error during execution: {e}")
            error_message = {
                "kind": "message",
                "messageId": str(uuid.uuid4()),
                "role": "agent",
                "parts": [
                    {
                        "kind": "text",
                        "text": f"I encountered an error processing your request: {e}",
                    }
                ],
                "contextId": context_id,
            }
            event_bus.publish(error_message)
            # Mark as failed
            event_bus.publish({
                "kind": "status-update",
                "taskId": task_id,
                "contextId": context_id,
                "status": {"state": "failed", "timestamp": datetime.utcnow().isoformat() + "Z"},
                "final": True,
            })
        finally:
            # Clean up cancellation set
            if task_id in self.cancelled_tasks:
                self.cancelled_tasks.remove(task_id)
            event_bus.finished()

    def _handle_cancelled(self, task_id, context_id, event_bus):
        print(f"[{task_id}] üõë Execution aborted due to cancellation.")
        event_bus.publish({
            "kind": "status-update",
            "taskId": task_id,
            "contextId": context_id,
            "status": {"state": "canceled", "timestamp": datetime.utcnow().isoformat() + "Z"},
            "final": True,
        })
        return

    def _generate_fallback_response(self, user_text: str) -> str:
        lower_text = user_text.lower()
        if "hello" in lower_text or "hi" in lower_text:
            return f"Hello! I'm {agent_card.name}, your AI assistant. How can I help you today?"
        if "help" in lower_text:
            return "I'm here to help! For better responses, configure an OpenAI API key."
        if "what" in lower_text and "do" in lower_text:
            return "I am an A2A-compliant AI agent. Try asking me questions!"
        return f'You said: "{user_text}". I am in simple mode. Configure OPENAI_API_KEY for AI responses.'

    async def cancel_task(self, task_id: str, event_bus):
        print(f"[{task_id}] üõë Task cancellation requested")
        self.cancelled_tasks.add(task_id)
        # Note: The execute loop picks up this flag. 
        # We don't publish the 'canceled' event here to avoid race conditions,
        # relying on execute() to publish it when it exits.
        



# Initialize A2A Server
app = FastAPI()
agent_executor = Executor()
task_store = InMemoryTaskStore()
request_handler = DefaultRequestHandler(
    task_store=task_store, agent_executor=agent_executor
)
app_builder = A2AFastAPIApplication(agent_card=agent_card, http_handler=request_handler)
app_builder.add_routes_to_app(app, agent_card_url=str(".agent-card.json"))

if __name__ == "__main__":
    port_str = os.environ.get("PORT")
    if port_str:
        port = int(port_str)
    else:
        parsed_url = urlparse(agent_card.url)
        port = parsed_url.port or 8082

    skill_names = ", ".join([s.id for s in agent_card.skills or []])

    print(f"üöÄ A2A Agent \"{agent_card.name}\" starting on port {port}")
    print(f"üìã Agent Card: http://localhost:{port}/.well-known/agent-card.json")
    print(f"üîß Skills: {skill_names}")
    print("\nüí° Try sending a message:")
    print("   - 'Hello'")
    print("   - 'What can you do?'\n")
    if not has_openai_key:
        print("‚ö†Ô∏è  No OPENAI_API_KEY found - running in simple mode")
        print("   Set OPENAI_API_KEY in .env for AI-powered responses\n")

    uvicorn.run(app, host="0.0.0.0", port=port)
